# ⚡ VERSION MINIMALE POUR HUGGING FACE INFERENCE API
# Uniquement les packages nécessaires pour Hugging Face (LLM externe)

# Core llama-index
llama-index-core>=0.10.0

# Hugging Face LLM (obligatoire pour l'API externe)
llama-index-llms-huggingface>=0.1.0

# Embeddings et vector store
llama-index-embeddings-huggingface>=0.1.0
llama-index-vector-stores-chroma>=0.1.0
llama-index-readers-file>=0.1.0

# ChromaDB - version avec wheels précompilés
chromadb>=0.4.22,<0.5.0

# Tokenizers - version spécifique avec wheels précompilés pour éviter la compilation Rust
tokenizers==0.15.2

# Embeddings et lecture PDF
sentence-transformers
pypdf
pymupdf

# API FastAPI
fastapi>=0.115.0
uvicorn[standard]>=0.32.0

# Hugging Face Hub (nécessaire pour l'Inference API)
huggingface-hub>=0.20.0

# NOTE: Packages exclus (non nécessaires pour Hugging Face uniquement) :
# - llama-cpp-python : Compilation très longue, modèle local uniquement
# - llama-index-llms-llama-cpp : Dépend de llama-cpp-python
# - llama-index-llms-openai : Si vous voulez utiliser OpenAI plus tard, ajoutez-le
# - llama-index-llms-anthropic : Si vous voulez utiliser Claude plus tard, ajoutez-le
# - llama-index-llms-ollama : Si vous voulez utiliser Ollama plus tard, ajoutez-le

# Configuration LLM Externe pour la RAG

Ce guide explique comment configurer l'API RAG pour utiliser un LLM externe au lieu d'un mod√®le local.

## Options disponibles

### 1. OpenAI (Recommand√© pour la production) ‚≠ê

**Avantages** : Rapide, fiable, bon support fran√ßais

**Configuration** :
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-votre-cle-api
OPENAI_MODEL=gpt-3.5-turbo  # ou gpt-4, gpt-4-turbo-preview
```

**Co√ªt** : ~$0.002 par requ√™te (gpt-3.5-turbo)

### 2. Anthropic Claude

**Avantages** : Excellent pour le fran√ßais, tr√®s performant

**Configuration** :
```bash
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-votre-cle-api
ANTHROPIC_MODEL=claude-3-haiku-20240307  # ou claude-3-sonnet-20240229
```

**Co√ªt** : ~$0.00025 par requ√™te (Claude Haiku)

### 3. Hugging Face Inference API

**Avantages** : Gratuit avec limitations, bon pour le d√©veloppement

**Configuration** :
```bash
LLM_PROVIDER=huggingface
HUGGINGFACE_API_KEY=hf_votre-cle-api
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

**Co√ªt** : Gratuit jusqu'√† un certain quota, puis payant

### 4. Ollama (Self-hosted)

**Avantages** : Gratuit, contr√¥le total, pas de limite

**Configuration** :
```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434  # ou URL de votre serveur Ollama
OLLAMA_MODEL=mistral  # ou llama2, codellama, etc.
```

**Co√ªt** : Gratuit (n√©cessite votre propre serveur)

## Configuration sur Render

### Variables d'environnement √† ajouter dans Render

1. Allez sur votre service RAG API dans Render
2. Cliquez sur "Environment"
3. Ajoutez les variables suivantes :

**Pour OpenAI** :
```
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-votre-cle-api
OPENAI_MODEL=gpt-3.5-turbo
```

**Pour Anthropic** :
```
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-votre-cle-api
ANTHROPIC_MODEL=claude-3-haiku-20240307
```

**Pour Hugging Face** :
```
LLM_PROVIDER=huggingface
HUGGINGFACE_API_KEY=hf_votre-cle-api
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

### Mise √† jour du render.yaml

Vous pouvez aussi ajouter ces variables dans `render.yaml` :

```yaml
  # API Python RAG
  - type: web
    name: rag-api
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn src.api.main_api:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: PORT
        value: 8002
      - key: CHROMA_DB_PATH
        value: /tmp/chroma_db
      - key: LLM_PROVIDER
        value: openai
      - key: OPENAI_API_KEY
        sync: false  # √Ä configurer manuellement dans Render
      - key: OPENAI_MODEL
        value: gpt-3.5-turbo
```

## Obtenir une cl√© API

### OpenAI
1. Allez sur https://platform.openai.com/api-keys
2. Cr√©ez un compte ou connectez-vous
3. Cr√©ez une nouvelle cl√© API
4. Ajoutez des cr√©dits (minimum $5)

### Anthropic
1. Allez sur https://console.anthropic.com/
2. Cr√©ez un compte
3. Allez dans "API Keys"
4. Cr√©ez une nouvelle cl√©

### Hugging Face
1. Allez sur https://huggingface.co/settings/tokens
2. Cr√©ez un compte ou connectez-vous
3. Cr√©ez un nouveau token avec les permissions "Read"
4. Pour l'Inference API, vous devrez peut-√™tre activer le paiement (gratuit jusqu'√† un certain quota)

## Test de la configuration

Apr√®s avoir configur√© les variables d'environnement, red√©ployez le service et v√©rifiez les logs. Vous devriez voir :

```
ü§ñ Utilisation d'OpenAI : gpt-3.5-turbo
‚úÖ LLM initialis√© avec succ√®s
```

## Fallback vers mod√®le local

Si aucun `LLM_PROVIDER` n'est configur√© ou si la configuration est incorrecte, le syst√®me utilisera automatiquement le mod√®le local (`LlamaCPP`) s'il est disponible dans `./data/llm_models/`.

## D√©pannage

**Erreur "OPENAI_API_KEY non d√©finie"** :
- V√©rifiez que la variable d'environnement est bien d√©finie dans Render
- Red√©ployez le service apr√®s avoir ajout√© la variable

**Erreur "Module not found"** :
- V√©rifiez que `requirements.txt` contient les d√©pendances n√©cessaires
- Red√©ployez le service pour installer les nouvelles d√©pendances

**R√©ponses lentes** :
- Essayez un mod√®le plus rapide (gpt-3.5-turbo au lieu de gpt-4)
- V√©rifiez votre connexion internet
- Pour Hugging Face, l'API peut √™tre lente selon la charge


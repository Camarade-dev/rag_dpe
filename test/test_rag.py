from llama_index.core import VectorStoreIndex, StorageContext, Settings, PromptTemplate
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.llama_cpp import LlamaCPP
import chromadb
import sys

# --- 1. CONFIGURATION DU LLM (MISTRAL) ---
print("‚è≥ Chargement du LLM...")
llm = LlamaCPP(
    # Assure-toi que le chemin est bon
    model_path="./data/llm_models/mistral-7b-instruct-v0.2.Q4_K_M.gguf", 
    temperature=0.1,
    max_new_tokens=1024, # On augmente un peu pour les r√©ponses longues
    context_window=4096,
    model_kwargs={"n_gpu_layers": 0},
    verbose=True # On met False pour avoir moins de blabla technique dans le terminal
)

# --- 2. CONFIGURATION EMBEDDINGS ---
embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

Settings.llm = llm
Settings.embed_model = embed_model

# --- 3. CONNEXION A LA BASE CHROMA ---
print("‚è≥ Connexion √† la base de donn√©es...")
db = chromadb.PersistentClient(path="./data/chroma_db")
chroma_collection = db.get_or_create_collection("renovation_knowledge")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# On enl√®ve persist_dir pour √©viter l'erreur docstore.json
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex.from_vector_store(
    vector_store,
    storage_context=storage_context,
)

# --- 4. D√âFINITION DU PROMPT (POUR FORCER LE FRAN√áAIS) ---
# C'est ici qu'on donne l'ordre strict au mod√®le
template_fr = (
    "Tu es un assistant expert en r√©novation √©nerg√©tique et b√¢timent (normes DTU, CPT).\n"
    "Utilise les informations de contexte ci-dessous pour r√©pondre √† la question.\n"
    "Si tu ne connais pas la r√©ponse, dis simplement que tu ne sais pas.\n"
    "IMPORTANT : R√©ponds imp√©rativement en FRAN√áAIS.\n"
    "---------------------\n"
    "CONTEXTE :\n"
    "{context_str}\n"
    "---------------------\n"
    "QUESTION : {query_str}\n"
    "R√âPONSE :"
)
qa_template = PromptTemplate(template_fr)

# --- 5. CR√âATION DU MOTEUR AVEC STREAMING ---
query_engine = index.as_query_engine(
    text_qa_template=qa_template, # On applique notre template fran√ßais
    streaming=True,               # On active le mode "machine √† √©crire"
    similarity_top_k=3            # On lit les 3 meilleurs passages trouv√©s
)

# --- 6. TEST ---
question = "Quelles sont les conditions pour l'√©vacuation des fum√©es d'une chaudi√®re fioul √©tanche ?"
print(f"\n‚ùì Question : {question}\n")
print("üí° R√©ponse en cours de g√©n√©ration...\n")

# Lancement de la requ√™te
response = query_engine.query(question)

# Affichage en direct (Streaming)
response.print_response_stream()
print("\n") # Petit saut de ligne √† la fin
import sys
import os  # <--- Ajout√© pour lire le fichier
import warnings
# D√©sactiver les warnings non-critiques
warnings.filterwarnings("ignore", category=UserWarning)
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
# D√©sactiver compl√®tement la t√©l√©m√©trie ChromaDB (plusieurs m√©thodes)
os.environ["CHROMA_TELEMETRY_DISABLED"] = "1"
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["ALLOW_RESET"] = "TRUE"

# Intercepter les erreurs de t√©l√©m√©trie ChromaDB (bug connu)
import logging
logging.getLogger("chromadb.telemetry").setLevel(logging.CRITICAL)

from llama_index.core import VectorStoreIndex, StorageContext, Settings, PromptTemplate
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import chromadb

# Import conditionnel des LLMs externes avec gestion d'erreurs robuste
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower()  # openai, huggingface, anthropic, ollama

# Imports conditionnels avec gestion d'erreurs - On essaie d'importer tous les packages disponibles
# pour permettre de changer de provider via les variables d'environnement
OpenAI = None
Anthropic = None
Ollama = None
LlamaCPP = None
HuggingFaceInferenceAPI = None
HuggingFaceLLM = None

# Essayer d'importer tous les packages (certains peuvent ne pas √™tre install√©s)
try:
    from llama_index.llms.openai import OpenAI
except ImportError:
    pass

try:
    from llama_index.llms.huggingface import HuggingFaceInferenceAPI
except ImportError:
    pass

try:
    from llama_index.llms.huggingface import HuggingFaceLLM
except ImportError:
    pass

try:
    from llama_index.llms.anthropic import Anthropic
except ImportError:
    pass

try:
    from llama_index.llms.ollama import Ollama
except ImportError:
    pass

try:
    from llama_index.llms.llama_cpp import LlamaCPP
except ImportError:
    pass

# Chemins - Utiliser des chemins absolus bas√©s sur le r√©pertoire du script
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DB_PATH = os.getenv("CHROMA_DB_PATH", os.path.join(BASE_DIR, "data", "chroma_db"))
# Chemin du mod√®le LLM local (utilis√© uniquement si LLM_PROVIDER n'est pas configur√©)
MODEL_PATH = os.getenv("LLM_MODEL_PATH", os.path.join(BASE_DIR, "data", "llm_models", "mistral-7b-instruct-v0.2.Q4_K_M.gguf"))
COLLECTION_NAME = "renovation_knowledge"
PROMPT_PATH = os.path.join(BASE_DIR, "prompts", "renovation_expert.txt")

class RenovationRAG: 
    def __init__(self): 
        print("üîß Initialisation du moteur RAG...")
        self._init_llm()
        self._init_embedding()
        self._init_vector_store()
        self._init_query_engine()
        print("‚úÖ Moteur RAG pr√™t √† l'emploi !")

    def _init_llm(self):
        """Charge le LLM (externe ou local selon la configuration)"""
        provider = LLM_PROVIDER
         
        if provider == "openai":
            if OpenAI is None:
                raise ImportError("‚ùå Package llama-index-llms-openai non install√©. Installez-le avec: pip install llama-index-llms-openai")
            
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("‚ùå OPENAI_API_KEY non d√©finie. Configurez-la dans les variables d'environnement.")
            
            model_name = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
            print(f"ü§ñ Utilisation d'OpenAI : {model_name}")
            self.llm = OpenAI(
                api_key=api_key,
                model=model_name,
                temperature=0.1,
                max_tokens=1024
            )
            
        elif provider == "huggingface":
            api_key = os.getenv("HUGGINGFACE_API_KEY")
            model_name = os.getenv("HUGGINGFACE_MODEL", "mistralai/Mistral-7B-Instruct-v0.2")
            
            if not api_key:
                raise ValueError("‚ùå HUGGINGFACE_API_KEY non d√©finie. Configurez-la dans les variables d'environnement pour utiliser l'API Inference (gratuit et sans RAM).")
            
            if HuggingFaceInferenceAPI is not None:
                print(f"ü§ñ Utilisation de Hugging Face Inference API : {model_name}")
                print(f"üîë API Key d√©tect√©e : {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else '***'}")
                try:
                    self.llm = HuggingFaceInferenceAPI(
                        model_name=model_name,
                        token=api_key,
                        temperature=0.1,
                        max_new_tokens=1024
                    )
                except Exception as e:
                    raise RuntimeError(f"‚ùå Erreur lors de l'initialisation de Hugging Face Inference API : {e}\n"
                                     f"üí° V√©rifiez que votre cl√© API est valide et que le mod√®le {model_name} est accessible.")
            elif HuggingFaceLLM is not None:
                # Fallback vers mod√®le local Hugging Face (n√©cessite plus de RAM)
                print(f"‚ö†Ô∏è  HuggingFaceInferenceAPI non disponible, utilisation du mod√®le local : {model_name}")
                print("‚ö†Ô∏è  ATTENTION: Le mod√®le sera charg√© localement (n√©cessite beaucoup de RAM)")
                self.llm = HuggingFaceLLM(
                    model_name=model_name,
                    temperature=0.1,
                    max_new_tokens=1024,
                    context_window=4096
                )
            else:
                raise ImportError("‚ùå Package llama-index-llms-huggingface non install√©.\n"
                                "üí° Installez-le avec: pip install llama-index-llms-huggingface huggingface-hub")
                
        elif provider == "anthropic":
            if Anthropic is None:
                raise ImportError("‚ùå Package llama-index-llms-anthropic non install√©. Installez-le avec: pip install llama-index-llms-anthropic")
            
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("‚ùå ANTHROPIC_API_KEY non d√©finie. Configurez-la dans les variables d'environnement.")
            
            model_name = os.getenv("ANTHROPIC_MODEL", "claude-3-haiku-20240307")
            print(f"ü§ñ Utilisation d'Anthropic Claude : {model_name}")
            self.llm = Anthropic(
                api_key=api_key,
                model=model_name,
                temperature=0.1,
                max_tokens=1024
            )
            
        elif provider == "ollama":
            if Ollama is None:
                raise ImportError("‚ùå Package llama-index-llms-ollama non install√©. Installez-le avec: pip install llama-index-llms-ollama")
            
            base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            model_name = os.getenv("OLLAMA_MODEL", "mistral")
            print(f"ü§ñ Utilisation d'Ollama : {model_name} ({base_url})")
            self.llm = Ollama(
                model=model_name,
                base_url=base_url,
                temperature=0.1,
                request_timeout=120.0
            )
            
        else:
            # Fallback vers mod√®le local LlamaCPP
            if LlamaCPP is None:
                raise ImportError("‚ùå Package llama-index-llms-llama-cpp non install√©. Installez-le avec: pip install llama-index-llms-llama-cpp")
            
            if not os.path.exists(MODEL_PATH):
                raise FileNotFoundError(
                    f"‚ùå Mod√®le local introuvable : {MODEL_PATH}\n"
                    f"üìÅ Placez votre fichier .gguf dans : {os.path.dirname(MODEL_PATH)}\n"
                    f"üí° Ou configurez un LLM externe avec LLM_PROVIDER (openai, huggingface, anthropic, ollama)"
                )
            
            print(f"ü§ñ Chargement du mod√®le local : {os.path.basename(MODEL_PATH)}")
            self.llm = LlamaCPP(
                model_path=MODEL_PATH,
                temperature=0.1,
                max_new_tokens=1024,
                context_window=4096,
                model_kwargs={"n_gpu_layers": 0},
                verbose=False
            )
        
        Settings.llm = self.llm
        print("‚úÖ LLM initialis√© avec succ√®s")

    def _init_embedding(self):
        """Charge le mod√®le de vectorisation"""
        self.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
        Settings.embed_model = self.embed_model

    def _init_vector_store(self):
        """Connexion √† ChromaDB"""
        # La t√©l√©m√©trie est d√©sactiv√©e via les variables d'environnement
        # Cr√©er le dossier si n√©cessaire
        os.makedirs(DB_PATH, exist_ok=True)
        db = chromadb.PersistentClient(path=DB_PATH)
        chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        self.storage_context = StorageContext.from_defaults(vector_store=vector_store)

    def _init_query_engine(self):
        """Configure le prompt depuis un fichier et le moteur de recherche"""
        index = VectorStoreIndex.from_vector_store(
            self.storage_context.vector_store,
            storage_context=self.storage_context,
        )

        # --- NOUVEAU CODE : Lecture du fichier txt ---
        if not os.path.exists(PROMPT_PATH):
            raise FileNotFoundError(f"‚ùå Le fichier de prompt est introuvable : {PROMPT_PATH}")

        with open(PROMPT_PATH, "r", encoding="utf-8") as f:
            template_content = f.read()
        
        # On v√©rifie que les variables obligatoires sont bien dans le texte
        if "{context_str}" not in template_content or "{query_str}" not in template_content:
            raise ValueError("‚ùå Le fichier prompt doit contenir {context_str} et {query_str}")

        qa_template = PromptTemplate(template_content)
        # ---------------------------------------------

        self.query_engine = index.as_query_engine(
            text_qa_template=qa_template,
            streaming=True,
            similarity_top_k=3 # Limite de 3 documents pour plus de contexte
        )

    def query(self, user_question):
        """M√©thode publique pour poser une question"""
        return self.query_engine.query(user_question)